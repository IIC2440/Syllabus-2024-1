{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c75c9dd",
   "metadata": {},
   "source": [
    "# Actividad PySpark - Parte 2\n",
    "\n",
    "En esta actividad vamos a evaluar que sepas escribir algoritmos bajo el paradigma de computación \"Map-Reduce\". Primero tendrás que escribir código en PySpark y después explicar ciertos puntos de cómo funciona tu código en un entorno distribuido.\n",
    "\n",
    "## Parte 1 - Escribiendo procedimientos en PySpark\n",
    "\n",
    "En esta actividad vamos a trabajar con los datos de la actividad de BigQuery. Estos datos son los que representan el caso ficticio de una tienda online que vende frutas. Las tablas disponibilizadas son `Usuarios`, `Frutas` y `Compras`, y las puedes encontrar en el GitHub junto a este archivo. Recordemos que las compras disponibles en los datos son solo para el mes de febrero.\n",
    "\n",
    "Para esta parte de la actividad vas a tener que escribir código en `pyspark` en celdas de código. Vas a tener permitido solamente usar las funciones más básicas de Spark, asociadas a [la API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html) de los objetos `RDD`. Por lo mismo, **no puedes cargar los datos en objetos de Python y hacer análisis locales**, ya que tu código debería funcionar para un conjunto de datos más grande que está en un sistema de archivos distribuido.\n",
    "\n",
    "### Consultas\n",
    "\n",
    "Tienes que escribir procedimientos en PySpark que respondan las siguientes consultas:\n",
    "\n",
    "1. Entrega todas las frutas con un precio unitario mayor o igual a 15.\n",
    "2. Entrega cada id de compra, junto con el nombre de la persona que la realizó, junto con la fruta que compró. Puedes pensar en retornar una fila por cada fruta en una compra.\n",
    "3. Entrega el el id de cada compra, junto al monto de cada compra.\n",
    "4. Para cada día, entrega el id y el monto de la compra más cara."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29280b3",
   "metadata": {},
   "source": [
    "## Explicando cómo se procesa cada consulta\n",
    "\n",
    "Ahora, pensando en un entorno distribuido donde hay muchos datos, explica cómo se resuelve cada uno de las consultas. En particular, queremos que te enfoques en cómo Spark saca ventaja de contar con varios \"workers\" en un entorno distribuido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
